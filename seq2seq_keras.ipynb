{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
    "#我的graphviz环境没配好，为了后面的Plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input,n_output,n_units):\n",
    "    #训练阶段\n",
    "    #encoder\n",
    "    encoder_input = Input(shape = (None, n_input))\n",
    "    #encoder输入维度n_input为每个时间步的输入xt的维度，这里是用来one-hot的英文字符数\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    #n_units为LSTM单元中每个门的神经元的个数，return_state设为True时才会返回最后时刻的状态h,c\n",
    "    _,encoder_h,encoder_c = encoder(encoder_input)\n",
    "    encoder_state = [encoder_h,encoder_c]\n",
    "    #保留下来encoder的末状态作为decoder的初始状态\n",
    "    \n",
    "    #decoder\n",
    "    decoder_input = Input(shape = (None, n_output))\n",
    "    #decoder的输入维度为中文字符数\n",
    "    decoder = LSTM(n_units,return_sequences=True, return_state=True)\n",
    "    #训练模型时需要decoder的输出序列来与结果对比优化，故return_sequences也要设为True\n",
    "    decoder_output, _, _ = decoder(decoder_input,initial_state=encoder_state)\n",
    "    #在训练阶段只需要用到decoder的输出序列，不需要用最终状态h.c\n",
    "    decoder_dense = Dense(n_output,activation='softmax')\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "    #输出序列经过全连接层得到结果\n",
    "    \n",
    "    #生成的训练模型\n",
    "    model = Model([encoder_input,decoder_input],decoder_output)\n",
    "    #第一个参数为训练模型的输入，包含了encoder和decoder的输入，第二个参数为模型的输出，包含了decoder的输出\n",
    "    \n",
    "    #推理阶段，用于预测过程\n",
    "    #推断模型—encoder\n",
    "    encoder_infer = Model(encoder_input,encoder_state)\n",
    "    \n",
    "    #推断模型-decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))    \n",
    "    decoder_state_input = [decoder_state_input_h, decoder_state_input_c]#上个时刻的状态h,c   \n",
    "    \n",
    "    decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(decoder_input,initial_state=decoder_state_input)\n",
    "    decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]#当前时刻得到的状态\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)#当前时刻的输出\n",
    "    decoder_infer = Model([decoder_input]+decoder_state_input,[decoder_infer_output]+decoder_infer_state)\n",
    "    \n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNITS = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "NUM_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/cmn.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(data_path,header=None).iloc[:NUM_SAMPLES,:,]\n",
    "df.columns=['inputs','targets']\n",
    "\n",
    "df['targets'] = df['targets'].apply(lambda x: '\\t'+x+'\\n')\n",
    "\n",
    "input_texts = df.inputs.values.tolist()\n",
    "target_texts = df.targets.values.tolist()\n",
    "\n",
    "input_characters = sorted(list(set(df.inputs.unique().sum())))\n",
    "target_characters = sorted(list(set(df.targets.unique().sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INUPT_LENGTH = max([len(i) for i in input_texts])\n",
    "OUTPUT_LENGTH = max([len(i) for i in target_texts])\n",
    "INPUT_FEATURE_LENGTH = len(input_characters)\n",
    "OUTPUT_FEATURE_LENGTH = len(target_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.zeros((NUM_SAMPLES,INUPT_LENGTH,INPUT_FEATURE_LENGTH))\n",
    "decoder_input = np.zeros((NUM_SAMPLES,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH))\n",
    "decoder_output = np.zeros((NUM_SAMPLES,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {char:index for index,char in enumerate(input_characters)}\n",
    "input_dict_reverse = {index:char for index,char in enumerate(input_characters)}\n",
    "target_dict = {char:index for index,char in enumerate(target_characters)}\n",
    "target_dict_reverse = {index:char for index,char in enumerate(target_characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index,seq in enumerate(input_texts):\n",
    "    for char_index, char in enumerate(seq):\n",
    "        encoder_input[seq_index,char_index,input_dict[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index,seq in enumerate(target_texts):\n",
    "    for char_index,char in enumerate(seq):\n",
    "        decoder_input[seq_index,char_index,target_dict[char]] = 1.0\n",
    "        if char_index > 0:\n",
    "            decoder_output[seq_index,char_index-1,target_dict[char]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察向量化的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([input_dict_reverse[np.argmax(i)] for i in encoder_input[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'嗨。\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([target_dict_reverse[np.argmax(i)] for i in decoder_output[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t嗨。\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([target_dict_reverse[np.argmax(i)] for i in decoder_input[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train, encoder_infer, decoder_infer = create_model(INPUT_FEATURE_LENGTH, OUTPUT_FEATURE_LENGTH, N_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#查看模型结构\n",
    "plot_model(to_file='model.png',model=model_train,show_shapes=True)\n",
    "plot_model(to_file='encoder.png',model=encoder_infer,show_shapes=True)\n",
    "plot_model(to_file='decoder.png',model=decoder_infer,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 2623)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 337920      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  2949120     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2623)   674111      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,961,151\n",
      "Trainable params: 3,961,151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 73)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 256), (None, 256) 337920    \n",
      "=================================================================\n",
      "Total params: 337,920\n",
      "Trainable params: 337,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_infer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 2623)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  2949120     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2623)   674111      lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,623,231\n",
      "Trainable params: 3,623,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_infer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 2.0315 - val_loss: 2.4918\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 1.9013 - val_loss: 2.4023\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 1.8018 - val_loss: 2.3162\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 148s 18ms/step - loss: 1.7133 - val_loss: 2.2307\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 1.6322 - val_loss: 2.1506\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 1.5603 - val_loss: 2.0892\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 1.4961 - val_loss: 2.0370\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 1.4367 - val_loss: 1.9959\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 1.3825 - val_loss: 1.9563\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 1.3326 - val_loss: 1.9192\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 1.2869 - val_loss: 1.8930\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 148s 18ms/step - loss: 1.2455 - val_loss: 1.8603\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 1.2060 - val_loss: 1.8538\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 1.1723 - val_loss: 1.8332\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 1.1395 - val_loss: 1.8263\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 1.1069 - val_loss: 1.8087\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 1.0779 - val_loss: 1.8020\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 1.0493 - val_loss: 1.7957\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 1.0213 - val_loss: 1.7862\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.9943 - val_loss: 1.7801\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.9675 - val_loss: 1.7878\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.9428 - val_loss: 1.7819\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.9180 - val_loss: 1.7863\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.8942 - val_loss: 1.7846\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.8714 - val_loss: 1.7808\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.8478 - val_loss: 1.7867\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.8253 - val_loss: 1.7896\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.8046 - val_loss: 1.7879\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.7839 - val_loss: 1.7874\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.7624 - val_loss: 1.7917\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.7422 - val_loss: 1.8001\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.7237 - val_loss: 1.7996\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.7047 - val_loss: 1.8062\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.6858 - val_loss: 1.8099\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.6684 - val_loss: 1.8266\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.6512 - val_loss: 1.8152\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.6343 - val_loss: 1.8272\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.6172 - val_loss: 1.8360\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.6017 - val_loss: 1.8407\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.5854 - val_loss: 1.8529\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 148s 19ms/step - loss: 0.5695 - val_loss: 1.8555\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.5545 - val_loss: 1.8624\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.5404 - val_loss: 1.8730\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.5261 - val_loss: 1.8786\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.5124 - val_loss: 1.8776\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.4982 - val_loss: 1.8904\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.4857 - val_loss: 1.9007\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.4726 - val_loss: 1.9110\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.4599 - val_loss: 1.9109\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.4478 - val_loss: 1.9309\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.4359 - val_loss: 1.9275\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 140s 18ms/step - loss: 0.4246 - val_loss: 1.9313\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.4123 - val_loss: 1.9422\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.4020 - val_loss: 1.9494\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 140s 17ms/step - loss: 0.3911 - val_loss: 1.9557\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.3810 - val_loss: 1.9653\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.3706 - val_loss: 1.9692\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.3609 - val_loss: 1.9818\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.3521 - val_loss: 1.9980\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 148s 18ms/step - loss: 0.3422 - val_loss: 1.9960\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.3334 - val_loss: 2.0059\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.3240 - val_loss: 2.0087\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.3159 - val_loss: 2.0132\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.3064 - val_loss: 2.0327\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.2984 - val_loss: 2.0442\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 148s 18ms/step - loss: 0.2900 - val_loss: 2.0480\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.2832 - val_loss: 2.0491\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.2754 - val_loss: 2.0573\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.2678 - val_loss: 2.0671\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.2607 - val_loss: 2.0622\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.2537 - val_loss: 2.0786\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.2471 - val_loss: 2.0893\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.2390 - val_loss: 2.0847\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.2328 - val_loss: 2.1010\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.2270 - val_loss: 2.1040\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.2207 - val_loss: 2.1171\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 144s 18ms/step - loss: 0.2145 - val_loss: 2.1367\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.2086 - val_loss: 2.1308\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 148s 18ms/step - loss: 0.2026 - val_loss: 2.1400\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.1972 - val_loss: 2.1458\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 141s 18ms/step - loss: 0.1913 - val_loss: 2.1459\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.1857 - val_loss: 2.1611\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 140s 17ms/step - loss: 0.1811 - val_loss: 2.1682\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.1752 - val_loss: 2.1757\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.1704 - val_loss: 2.1858\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.1651 - val_loss: 2.1916\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.1611 - val_loss: 2.1996\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 140s 17ms/step - loss: 0.1562 - val_loss: 2.1961\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 140s 18ms/step - loss: 0.1515 - val_loss: 2.2116\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.1474 - val_loss: 2.2186\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.1423 - val_loss: 2.2275\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 142s 18ms/step - loss: 0.1383 - val_loss: 2.2386\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 140s 18ms/step - loss: 0.1338 - val_loss: 2.2445\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.1310 - val_loss: 2.2439\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.1267 - val_loss: 2.2512\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.1232 - val_loss: 2.2649\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 145s 18ms/step - loss: 0.1186 - val_loss: 2.2577\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.1148 - val_loss: 2.2800\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.1120 - val_loss: 2.2841\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 140s 17ms/step - loss: 0.1087 - val_loss: 2.2885\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.1049 - val_loss: 2.2923\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.1016 - val_loss: 2.3073\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0979 - val_loss: 2.3077\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0964 - val_loss: 2.3094\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0920 - val_loss: 2.3266\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0899 - val_loss: 2.3256\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 140s 18ms/step - loss: 0.0872 - val_loss: 2.3311\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0837 - val_loss: 2.3412\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0814 - val_loss: 2.3459\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0789 - val_loss: 2.3480\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0763 - val_loss: 2.3553\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0739 - val_loss: 2.3560\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0717 - val_loss: 2.3704\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0683 - val_loss: 2.3709\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0675 - val_loss: 2.3795\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0649 - val_loss: 2.3779\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0635 - val_loss: 2.3913\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0613 - val_loss: 2.3940\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0593 - val_loss: 2.4052\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0565 - val_loss: 2.4161\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0553 - val_loss: 2.4181\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0532 - val_loss: 2.4218\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0522 - val_loss: 2.4329\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0501 - val_loss: 2.4339\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0488 - val_loss: 2.4347\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0469 - val_loss: 2.4427\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0458 - val_loss: 2.4543\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0444 - val_loss: 2.4585\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0427 - val_loss: 2.4758\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0420 - val_loss: 2.4726\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0410 - val_loss: 2.4760\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0389 - val_loss: 2.4778\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0382 - val_loss: 2.4895\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0367 - val_loss: 2.4886\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0361 - val_loss: 2.4861\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.0351 - val_loss: 2.5039\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0347 - val_loss: 2.5010\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0331 - val_loss: 2.5037\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0325 - val_loss: 2.5184\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0313 - val_loss: 2.5136\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0308 - val_loss: 2.5215\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0292 - val_loss: 2.5212\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.0298 - val_loss: 2.5272\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0284 - val_loss: 2.5382\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0280 - val_loss: 2.5381\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0272 - val_loss: 2.5389\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0266 - val_loss: 2.5433\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0253 - val_loss: 2.5487\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0252 - val_loss: 2.5608\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0247 - val_loss: 2.5524\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0240 - val_loss: 2.5748\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 135s 17ms/step - loss: 0.0237 - val_loss: 2.5638\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0234 - val_loss: 2.5648\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0224 - val_loss: 2.5735\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0219 - val_loss: 2.5742\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0216 - val_loss: 2.5764\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0212 - val_loss: 2.5900\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0208 - val_loss: 2.5951\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0204 - val_loss: 2.5881\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0201 - val_loss: 2.5881\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 135s 17ms/step - loss: 0.0199 - val_loss: 2.6009\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0192 - val_loss: 2.5970\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0191 - val_loss: 2.6028\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0183 - val_loss: 2.6124\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0183 - val_loss: 2.6013\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0180 - val_loss: 2.6025\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0178 - val_loss: 2.6097\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0174 - val_loss: 2.6170\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0182 - val_loss: 2.6280\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0169 - val_loss: 2.6386\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0163 - val_loss: 2.6378\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.0169 - val_loss: 2.6365\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0159 - val_loss: 2.6389\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0157 - val_loss: 2.6448\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0158 - val_loss: 2.6511\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0155 - val_loss: 2.6509\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0158 - val_loss: 2.6675\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0155 - val_loss: 2.6486\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.0146 - val_loss: 2.6504\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0147 - val_loss: 2.6622\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0143 - val_loss: 2.6563\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0140 - val_loss: 2.6610\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 135s 17ms/step - loss: 0.0145 - val_loss: 2.6636\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0143 - val_loss: 2.6708\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0136 - val_loss: 2.6750\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 139s 17ms/step - loss: 0.0137 - val_loss: 2.6766\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 135s 17ms/step - loss: 0.0137 - val_loss: 2.6705\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0132 - val_loss: 2.6714\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0133 - val_loss: 2.6841\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0130 - val_loss: 2.6885\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0134 - val_loss: 2.6900\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0131 - val_loss: 2.6828\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0132 - val_loss: 2.6922\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.0128 - val_loss: 2.7136\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0127 - val_loss: 2.6979\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0130 - val_loss: 2.6985\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0122 - val_loss: 2.7076\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0122 - val_loss: 2.7070\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 136s 17ms/step - loss: 0.0129 - val_loss: 2.7023\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 138s 17ms/step - loss: 0.0120 - val_loss: 2.7076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bc9234f7f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit([encoder_input,decoder_input],decoder_output,batch_size=BATCH_SIZE,epochs=EPOCH,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source,encoder_inference, decoder_inference, n_steps, features):\n",
    "    #先通过推理encoder获得预测输入序列的隐状态\n",
    "    state = encoder_inference.predict(source)\n",
    "    #第一个字符'\\t',为起始标志\n",
    "    predict_seq = np.zeros((1,1,features))\n",
    "    predict_seq[0,0,target_dict['\\t']] = 1\n",
    "\n",
    "    output = ''\n",
    "    #开始对encoder获得的隐状态进行推理\n",
    "    #每次循环用上次预测的字符作为输入来预测下一次的字符，直到预测出了终止符\n",
    "    for i in range(n_steps):#n_steps为句子最大长度\n",
    "        #给decoder输入上一个时刻的h,c隐状态，以及上一次的预测字符predict_seq\n",
    "        yhat,h,c = decoder_inference.predict([predict_seq]+state)\n",
    "        #注意，这里的yhat为Dense之后输出的结果，因此与h不同\n",
    "        char_index = np.argmax(yhat[0,-1,:])\n",
    "        char = target_dict_reverse[char_index]\n",
    "        output += char\n",
    "        state = [h,c]#本次状态做为下一次的初始状态继续传递\n",
    "        predict_seq = np.zeros((1,1,features))\n",
    "        predict_seq[0,0,char_index] = 1\n",
    "        if char == '\\n':#预测到了终止符则停下来\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have brothers.\n",
      "我有兄弟。\n",
      "\n",
      "I have ten pens.\n",
      "我有十支筆。\n",
      "\n",
      "I have to hurry!\n",
      "我要赶紧了!\n",
      "\n",
      "I have two cats.\n",
      "我有两只猫。\n",
      "\n",
      "I have two sons.\n",
      "我有兩個兒子。\n",
      "\n",
      "I just threw up.\n",
      "我剛才吐了。\n",
      "\n",
      "I lent him a CD.\n",
      "我借给他一盘CD。\n",
      "\n",
      "I like Tom, too.\n",
      "我也喜歡湯姆。\n",
      "\n",
      "I like football.\n",
      "我喜歡足球。\n",
      "\n",
      "I like potatoes.\n",
      "我喜歡土豆。\n",
      "\n",
      "I like the cold.\n",
      "我喜歡寒冷。\n",
      "\n",
      "I like this dog.\n",
      "我喜欢这只狗。\n",
      "\n",
      "I like your car.\n",
      "我喜欢您的车。\n",
      "\n",
      "I lived in Rome.\n",
      "我住在羅馬。\n",
      "\n",
      "I love this car.\n",
      "我愛這台車。\n",
      "\n",
      "I might say yes.\n",
      "我可能会说是。\n",
      "\n",
      "I must help her.\n",
      "我必須幫助她。\n",
      "\n",
      "I need a friend.\n",
      "我需要个朋友。\n",
      "\n",
      "I need evidence.\n",
      "我需要證據。\n",
      "\n",
      "I need you here.\n",
      "我需要你在這裡。\n",
      "\n",
      "I paid the bill.\n",
      "我买了单。\n",
      "\n",
      "I played tennis.\n",
      "我打網球了。\n",
      "\n",
      "I run every day.\n",
      "我每天跑步。\n",
      "\n",
      "I speak Swedish.\n",
      "我说瑞典语。\n",
      "\n",
      "I talked to her.\n",
      "我跟她谈了话。\n",
      "\n",
      "I teach Chinese.\n",
      "我教中文。\n",
      "\n",
      "I think it's OK.\n",
      "我想沒關係。\n",
      "\n",
      "I took a shower.\n",
      "我洗了澡。\n",
      "\n",
      "I want a guitar.\n",
      "我想要一把吉他。\n",
      "\n",
      "I want that bag.\n",
      "我想要那個袋子。\n",
      "\n",
      "I want to drive.\n",
      "我想開車。\n",
      "\n",
      "I was surprised.\n",
      "我吃惊了。\n",
      "\n",
      "I wish you'd go.\n",
      "我希望你去。\n",
      "\n",
      "I woke up early.\n",
      "我起得早。\n",
      "\n",
      "I work too much.\n",
      "我工作得太多了。\n",
      "\n",
      "I'll bring wine.\n",
      "我会带酒来。\n",
      "\n",
      "I'll never stop.\n",
      "我絕不會停。\n",
      "\n",
      "I'm a foreigner.\n",
      "我是一個外國人。\n",
      "\n",
      "I'm a night owl.\n",
      "我是個夜貓子。\n",
      "\n",
      "I'm about ready.\n",
      "我快好了。\n",
      "\n",
      "I'm always here.\n",
      "我一直在這裡。\n",
      "\n",
      "I'm daydreaming.\n",
      "我在做白日梦。\n",
      "\n",
      "I'm feeling fit.\n",
      "我覺得精神很好。\n",
      "\n",
      "I'm left-handed.\n",
      "我是左撇子。\n",
      "\n",
      "I'm not serious.\n",
      "我不是认真的。\n",
      "\n",
      "I'm out of time.\n",
      "我没时间了。\n",
      "\n",
      "I'm really busy.\n",
      "我真的好忙。\n",
      "\n",
      "I'm really cold.\n",
      "我真的冷。\n",
      "\n",
      "I'm still angry.\n",
      "我还饿着呢。\n",
      "\n",
      "I'm very hungry.\n",
      "我很餓。\n",
      "\n",
      "I'm very lonely.\n",
      "我很寂寞。\n",
      "\n",
      "I've had enough.\n",
      "我已經受夠了。\n",
      "\n",
      "I've had enough.\n",
      "我已經受夠了。\n",
      "\n",
      "Is Tom Canadian?\n",
      "Tom是我高。\n",
      "\n",
      "Is he breathing?\n",
      "他在呼吸嗎?\n",
      "\n",
      "Is it all there?\n",
      "全都在那裡嗎？\n",
      "\n",
      "Is it too salty?\n",
      "还有多余的盐吗？\n",
      "\n",
      "Is she Japanese?\n",
      "她是日本人嗎？\n",
      "\n",
      "Is this a river?\n",
      "這是一條河嗎?\n",
      "\n",
      "Isn't that mine?\n",
      "那是我的吗？\n",
      "\n",
      "It is up to you.\n",
      "由你來決定。\n",
      "\n",
      "It snowed a lot.\n",
      "下了很多的雪。\n",
      "\n",
      "It was terrible.\n",
      "真糟糕。\n",
      "\n",
      "It was very far.\n",
      "它很遠。\n",
      "\n",
      "It'll be cloudy.\n",
      "天要变多云了。\n",
      "\n",
      "It's a dead end.\n",
      "这是个死胡同。\n",
      "\n",
      "It's a new book.\n",
      "那本書是一本新書。\n",
      "\n",
      "It's a nice day.\n",
      "今天天氣很好。\n",
      "\n",
      "It's a surprise.\n",
      "这是一个惊喜。\n",
      "\n",
      "It's almost six.\n",
      "快要六點了。\n",
      "\n",
      "It's already 11.\n",
      "已经是11点了。\n",
      "\n",
      "It's fine today.\n",
      "今天天气很好。\n",
      "\n",
      "It's impossible.\n",
      "這是不可能的。\n",
      "\n",
      "It's lunch time.\n",
      "午餐時間到了。\n",
      "\n",
      "It's okay to go.\n",
      "你可以走了。\n",
      "\n",
      "It's over there.\n",
      "在那里。\n",
      "\n",
      "It's time to go.\n",
      "是該離開的時候了。\n",
      "\n",
      "It's time to go.\n",
      "是該離開的時候了。\n",
      "\n",
      "Jesus loves you.\n",
      "耶穌愛你。\n",
      "\n",
      "Keep on smiling.\n",
      "保持微笑。\n",
      "\n",
      "Keep on working.\n",
      "繼續工作！\n",
      "\n",
      "Keep the change!\n",
      "不用找零钱了。\n",
      "\n",
      "Large, isn't it?\n",
      "很大, 不是嗎?\n",
      "\n",
      "Lemons are sour.\n",
      "檸檬是酸的。\n",
      "\n",
      "Let me go alone.\n",
      "讓我一個人去。\n",
      "\n",
      "Let me see that.\n",
      "讓我看看。\n",
      "\n",
      "Let them decide.\n",
      "讓他們決定。\n",
      "\n",
      "Let's eat sushi.\n",
      "讓我們吃壽司吧。\n",
      "\n",
      "Let's go by bus.\n",
      "讓我們坐公共汽車去。\n",
      "\n",
      "Let's not argue.\n",
      "我們別吵了。\n",
      "\n",
      "Let's turn back.\n",
      "我们掉头吧！\n",
      "\n",
      "Look at the sky.\n",
      "看天上。\n",
      "\n",
      "Look behind you.\n",
      "瞧你身後。\n",
      "\n",
      "Make it smaller.\n",
      "把它弄小一點。\n",
      "\n",
      "May I leave now?\n",
      "我现在能走了吗？\n",
      "\n",
      "May I try it on?\n",
      "我能试一下吗？\n",
      "\n",
      "Maybe next time.\n",
      "也许下一次吧。\n",
      "\n",
      "Men should work.\n",
      "男人应该工作。\n",
      "\n",
      "Merry Christmas!\n",
      "聖誕快樂。\n",
      "\n",
      "Mom, I'm hungry.\n",
      "妈妈，我肚子饿了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,1100):\n",
    "    test = encoder_input[i:i+1,:,:]#i:i+1保持数组是三维\n",
    "    out = predict_chinese(test,encoder_infer,decoder_infer,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH)\n",
    "    #print(input_texts[i],'\\n---\\n',target_texts[i],'\\n---\\n',out)\n",
    "    print(input_texts[i])\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
